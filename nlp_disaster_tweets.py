# -*- coding: utf-8 -*-
"""submission-nlp-disaster-tweets

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1atVDeiHqiyv7CTyYIq1xTHVryaJbQ2Is

# SUBMISSION PROYEK PERTAMA: MEMBUAT MODEL NLP DENGAN TENSORFLOW

Oleh Fadilla Rizalul Yahya (github.com/fadillarizalul)

Pada submission ini, dilakukan pembuatan model untuk klasifikasi teks. Dataset yang digunakan merupakan dataset cuitan/twit mengenai gempa bumi yang diambil dari website Kaggle. Dataset tersebut berisi lebih dari 11.000 tweet yang terkait dengan kata kunci bencana seperti “crash”, “quarantine”, and “bush fires”, serta lokasi dan kata kunci itu sendiri.

## Outline:

*   Mengimpor library
*   Pra-pemrosesan data
*   Tokenizer
*   Buat arsitektur model
*   Training model
*   Plotting loss dan accuracy

# Mengimpor library yang digunakan

Pada cell pertama import library pandas dan ubah dataset menjadi dataframe. Lalu panggil fungsi .head() untuk melihat lima baris pertama dari dataset
"""

import pandas as pd

data = pd.read_csv('tweets.csv')
data.head()

"""# Melakukan pra-pemrosesan data

Kemudian buang kolom 'id' dan 'keyword' karena kita hanya akan menggunakan 'text' sebagai atribut untuk dilatih pada model.
"""

# menghilangkan kolom yang tidak berkaitan
data = data.drop(columns=['id', 'keyword'])

# melihat panjang kata dari kolom 'text'
data['text_length'] = data['text'].apply(lambda x: len(x.split(' ')))

# melihat lima baris pertama dari dataset
data.head()

"""Panggil .info() untuk melihat properties pada dataset"""

data.info()

"""Agar dapat diproses oleh model, kita perlu mengubah nilai-nilai dari dataframe ke dalam tipe data numpy array menggunakan atribut values."""

teks = data['text'].values
target = data['target'].values

teks

target

"""Lalu, bagi data untuk **training** dan data untuk testing."""

from sklearn.model_selection import train_test_split
teks_latih, teks_test, target_latih, target_test = train_test_split(teks, target, test_size=0.2)

"""# Melakukan proses Tokenizer

Kemudian kita ubah setiap kata pada dataset kita ke dalam bilangan numerik dengan fungsi **Tokenizer**. Setelah tokenisasi selesai, kita perlu membuat mengonversi setiap sampel menjadi **sequence**.
"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
 
tokenizer = Tokenizer(num_words=1000)
tokenizer.fit_on_texts(teks_latih) 
tokenizer.fit_on_texts(teks_test)
 
sekuens_latih = tokenizer.texts_to_sequences(teks_latih)
sekuens_test = tokenizer.texts_to_sequences(teks_test)
 
padded_latih = pad_sequences(sekuens_latih, padding='post')
padded_test = pad_sequences(sekuens_test, padding='post')

"""# Membuat Arsitektur Model

Untuk arsitektur model kita menggunakan **layer Embedding** dengan **dimensi embedding** sebesar 16, serta **dimensi dari input** sebesar nilai **num_words** pada objek tokenizer. Tidak lupa memanggil fungsi **compile** dan juga menentukan **optimizer** serta **loss function** yang akan dipakai oleh model. Ditambah dengan parameter **Dropout**  untuk mencegah overfitting pada model.
"""

import tensorflow as tf
from keras.layers import (LSTM, 
                          Embedding, 
                          BatchNormalization,
                          Dense, 
                          TimeDistributed, 
                          Dropout, 
                          Bidirectional,
                          Flatten, 
                          GlobalMaxPool1D)

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=1000, output_dim=16),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(1, activation='relu')
])
model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])

"""# Melakukan pelatihan model

Agar lebih menghemat waktu dalam proses pelatihan, kita menggunakan fungsi **Callback**. Fungsi callback membantu kita untuk memberi tahu model agar berhenti melakukan pelatihan ketika sudah mencapai target tertentu. Fungsi ini sangat berguna untuk menghemat waktu pelatihan dari model.

Terakhir kita dapat mulai **melatih** model kita dengan memanggil fungsi fit()
"""

# membuat fungsi call back
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.9):
    #if(logs.get('val_accuracy and accuracy')>0.88):
    #if(logs.get('accuracy') > 0.97 and logs.get('val_accuracy') > 0.97 ):  
      print("\nAkurasi telah mencapai 85%")
      self.model.stop_training = True

callbacks = myCallback()

# melakukan pelatihan model
num_epochs = 30
history = model.fit(padded_latih, target_latih, epochs=num_epochs, 
                    validation_data=(padded_test, target_test), verbose=2, callbacks=[callbacks])

"""# Membuat plot loss dan plot accuracy

Agar nilai loss dan nilai accuracy dapat tervisualisasi dengan jelas. Kita buat **plot untuk loss dan accuracy** dengan memilih  metrik accuracy pada fungsi history.
"""

import matplotlib.pyplot as plt

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper right')
plt.show()

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper right')
plt.show()